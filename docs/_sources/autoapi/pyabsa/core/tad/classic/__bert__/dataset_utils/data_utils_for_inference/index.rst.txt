:py:mod:`pyabsa.core.tad.classic.__bert__.dataset_utils.data_utils_for_inference`
=================================================================================

.. py:module:: pyabsa.core.tad.classic.__bert__.dataset_utils.data_utils_for_inference


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   pyabsa.core.tad.classic.__bert__.dataset_utils.data_utils_for_inference.Tokenizer4Pretraining
   pyabsa.core.tad.classic.__bert__.dataset_utils.data_utils_for_inference.BERTTADDataset




.. py:class:: Tokenizer4Pretraining(max_seq_len, opt, **kwargs)

   .. py:method:: text_to_sequence(text, reverse=False, padding='post', truncating='post')



.. py:class:: BERTTADDataset(tokenizer, opt)

   Bases: :py:obj:`torch.utils.data.Dataset`

   An abstract class representing a :class:`Dataset`.

   All datasets that represent a map from keys to data samples should subclass
   it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
   data sample for a given key. Subclasses could also optionally overwrite
   :meth:`__len__`, which is expected to return the size of the dataset by many
   :class:`~torch.utils.data.Sampler` implementations and the default options
   of :class:`~torch.utils.data.DataLoader`.

   .. note::
     :class:`~torch.utils.data.DataLoader` by default constructs a index
     sampler that yields integral indices.  To make it work with a map-style
     dataset with non-integral indices/keys, a custom sampler must be provided.

   .. py:method:: parse_sample(text)


   .. py:method:: prepare_infer_sample(text: str, ignore_error)


   .. py:method:: prepare_infer_dataset(infer_file, ignore_error)


   .. py:method:: process_data(samples, ignore_error=True)


   .. py:method:: __getitem__(index)


   .. py:method:: __len__()



