:py:mod:`pyabsa.core.tad.classic.__bert__.layers.squeeze_embedding`
===================================================================

.. py:module:: pyabsa.core.tad.classic.__bert__.layers.squeeze_embedding


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   pyabsa.core.tad.classic.__bert__.layers.squeeze_embedding.SqueezeEmbedding




.. py:class:: SqueezeEmbedding(batch_first=True)

   Bases: :py:obj:`torch.nn.Module`

   Squeeze sequence embedding length to the longest one in the batch

   .. py:method:: forward(x, x_len)

      sequence -> sort -> pad and pack -> unpack ->unsort
      :param x: sequence embedding vectors
      :param x_len: numpy/tensor list
      :return:



