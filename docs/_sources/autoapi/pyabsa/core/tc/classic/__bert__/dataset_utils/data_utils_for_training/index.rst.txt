:py:mod:`pyabsa.core.tc.classic.__bert__.dataset_utils.data_utils_for_training`
===============================================================================

.. py:module:: pyabsa.core.tc.classic.__bert__.dataset_utils.data_utils_for_training


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   pyabsa.core.tc.classic.__bert__.dataset_utils.data_utils_for_training.Tokenizer4Pretraining
   pyabsa.core.tc.classic.__bert__.dataset_utils.data_utils_for_training.BERTTCDataset




.. py:class:: Tokenizer4Pretraining(max_seq_len, opt)

   .. py:method:: text_to_sequence(text, reverse=False, padding='post', truncating='post')



.. py:class:: BERTTCDataset(dataset_list, tokenizer, opt)

   Bases: :py:obj:`torch.utils.data.Dataset`

   An abstract class representing a :class:`Dataset`.

   All datasets that represent a map from keys to data samples should subclass
   it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a
   data sample for a given key. Subclasses could also optionally overwrite
   :meth:`__len__`, which is expected to return the size of the dataset by many
   :class:`~torch.utils.data.Sampler` implementations and the default options
   of :class:`~torch.utils.data.DataLoader`.

   .. note::
     :class:`~torch.utils.data.DataLoader` by default constructs a index
     sampler that yields integral indices.  To make it work with a map-style
     dataset with non-integral indices/keys, a custom sampler must be provided.

   .. py:attribute:: bert_baseline_input_colses
      

      

   .. py:method:: __getitem__(index)


   .. py:method:: __len__()



